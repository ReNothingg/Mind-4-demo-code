model:
  hidden_size: 2048
  num_hidden_layers: 24
  num_attention_heads: 16
  num_key_value_heads: 4
  vocab_size: 50304
  head_dim: 128
  intermediate_size: 5632
  num_experts: 8
  experts_per_token: 2
  rope_theta: 10000.0
  sliding_window: 4096
  initial_context_length: 4096
  max_context_length: 131072
  rope_scaling_factor: 1.0
  rope_ntk_alpha: 1.0
  rope_ntk_beta: 32.0
  swiglu_limit: 1.0

  embedding_dropout: 0.1
  layer_norm_eps: 1e-5
  tie_word_embeddings: true

data:
  train_dataset: "train/train_data.txt"
  val_dataset: "evaluate/val_data.txt"
  tokenizer_path: "tokenizer/tokenizer.json"

  max_seq_length: 2048
  batch_size: 8
  num_workers: 4
  pin_memory: true

training:
  num_epochs: 3
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  warmup_steps: 1000
  logging_steps: 100
  eval_steps: 1000
  save_steps: 5000
  save_path: "checkpoints/mind_epoch_{epoch}.ckpt"
  resume_from: null

  optimizer:
    type: "AdamW"
    lr: 5e-5
    betas: [0.9, 0.95]
    weight_decay: 0.1
    eps: 1e-8

  scheduler:
    type: "cosine_with_warmup"
    min_lr: 1e-6
    total_steps: 100000

  distributed: false
  world_size: 1
  backend: "nccl"

  moe_aux_loss_coeff: 0.01

hardware:
  device: "cuda"
  mixed_precision: "bf16"
  gradient_checkpointing: true

eval:
  metrics:
    - perplexity
    - loss
  benchmarks:
    - mmlu
    - gsm8k
    - humaneval
    - truthfulqa

logging:
  wandb_project: "mind"
  wandb_entity: "renothingg"
  tensorboard: true

early_stopping:
  patience: 3
  min_delta: 0.001

hooks:
  - type: "checkpoint"
  - type: "evaluate"

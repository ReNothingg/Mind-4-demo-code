# –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è Mind-4 –≤ Google Colab

## –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π —Å–ø–æ—Å–æ–± –Ω–∞—á–∞—Ç—å

### –ö–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –≤ Google Colab (colab.research.google.com)

**–Ø—á–µ–π–∫–∞ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ**

```python
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch —Å CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤
!pip install pyyaml tqdm -q

# –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ git pull –µ—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å)
!git clone https://github.com/ReNothingg/Mind-4-demo-code.git 2>/dev/null || (cd Mind-4-demo-code && git pull)

# –ü–µ—Ä–µ—Ö–æ–¥–∏–º –≤ –ø–∞–ø–∫—É
%cd Mind-4-demo-code

# –ü—Ä–æ–≤–µ—Ä—è–µ–º GPU
import torch
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
```

**–Ø—á–µ–π–∫–∞ 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–≤—ã–±–∏—Ä–∏—Ç–µ –æ–¥–∏–Ω –≤–∞—Ä–∏–∞–Ω—Ç)**

```python
# ===== –í–ê–†–ò–ê–ù–¢ A: –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–≤–æ–π —Ñ–∞–π–ª =====
from google.colab import files
import shutil

print("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª train_data.txt")
uploaded = files.upload()

# –ü–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –≤ –Ω—É–∂–Ω—É—é –ø–∞–ø–∫—É
for filename in uploaded.keys():
    src = f"/content/{filename}"
    dst = "./train/train_data.txt"
    shutil.copy(src, dst)
    print(f"–§–∞–π–ª —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω: {dst}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–∫–æ–ø–∏–ª–æ—Å—å
!ls -lh ./train/train_data.txt
```

```python
# ===== –í–ê–†–ò–ê–ù–¢ B: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Google Drive =====
from google.colab import drive
import shutil

# –ú–æ–Ω—Ç–∏—Ä—É–µ–º Drive
drive.mount('/content/gdrive')

# –ö–æ–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
shutil.copy('/content/gdrive/My Drive/train_data.txt', './train/train_data.txt')
print("  –î–∞–Ω–Ω—ã–µ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω—ã —Å Google Drive")
```

```python
# ===== –í–ê–†–ò–ê–ù–¢ C: –°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ =====
# –î–ª—è —Ç–µ—Å—Ç–∞ —Å–æ–∑–¥–∞–¥–∏–º —Ñ–∞–π–ª —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
test_data = """
The quick brown fox jumps over the lazy dog.
Machine learning is fascinating and powerful.
Python is great for scientific computing.
Neural networks learn patterns from data.
Transformers have revolutionized NLP.
Deep learning requires lots of data and compute.
GPUs make training much faster.
This is a training example for the Mind-4 model.
Artificial intelligence is transforming the world.
Language models can generate impressive text.
""" * 100  # –ü–æ–≤—Ç–æ—Ä—è–µ–º —á—Ç–æ–±—ã –±—ã–ª–æ –ø–æ–±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö

with open('./train/train_data.txt', 'w') as f:
    f.write(test_data)

print(f"–°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª ({len(test_data)} —Å–∏–º–≤–æ–ª–æ–≤)")
```

**–Ø—á–µ–π–∫–∞ 3: –ú–∞–ª–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ T4)**

```python
import yaml

config_small = """
model:
  hidden_size: 512
  num_hidden_layers: 8
  num_attention_heads: 8
  num_key_value_heads: 2
  vocab_size: 50000
  head_dim: 64
  intermediate_size: 1024
  num_experts: 4
  experts_per_token: 2
  rope_theta: 10000.0
  sliding_window: 1024
  initial_context_length: 1024
  rope_scaling_factor: 1.0
  rope_ntk_alpha: 1.0
  rope_ntk_beta: 32.0
  swiglu_limit: 1.0

data:
  train_dataset: "./train/train_data.txt"
  val_dataset: null
  max_seq_length: 512
  batch_size: 2
  num_workers: 0
  pin_memory: false

training:
  num_epochs: 1
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
  warmup_steps: 50
  logging_steps: 10
  eval_steps: 100
  save_steps: 100
  save_path: "./checkpoints/mind_epoch_{epoch}.pt"

  optimizer:
    type: "AdamW"
    lr: 1e-4
    betas: [0.9, 0.95]
    weight_decay: 0.01
    eps: 1e-8

hardware:
  device: "cuda"
  mixed_precision: "fp16"
  gradient_checkpointing: true
"""

with open('./config/train_small.yaml', 'w') as f:
    f.write(config_small)

print("  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–π –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
```

**–Ø—á–µ–π–∫–∞ 4: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è**

```python
# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
!python colab_train.py \
    --config config/train_small.yaml \
    --data ./train/train_data.txt \
    --max-samples 1000 \
    --dry-run  # –£–±–∏—Ä–∏—Ç–µ —Ñ–ª–∞–≥ --dry-run –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
```

**–Ø—á–µ–π–∫–∞ 5: –°–∫–∞—á–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã**

```python
from google.colab import files
import os

# –°–∫–∞—á–∏–≤–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç—ã
checkpoint_dir = './checkpoints'
for file in os.listdir(checkpoint_dir):
    if file.endswith('.pt'):
        files.download(os.path.join(checkpoint_dir, file))
        print(f"–°–∫–∞—á–∞–Ω: {file}")
```

---

## –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –¥–µ–π—Å—Ç–≤–∏–π

1. **–Ø—á–µ–π–∫–∞ 1**: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ GPU
2. **–Ø—á–µ–π–∫–∞ 2**: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–≤—ã–±–µ—Ä–∏—Ç–µ –æ–¥–∏–Ω –≤–∞—Ä–∏–∞–Ω—Ç A, B –∏–ª–∏ C)
3. **–Ø—á–µ–π–∫–∞ 3**: –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
4. **–Ø—á–µ–π–∫–∞ 4**: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
5. **–Ø—á–µ–π–∫–∞ 5**: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

---

## ‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö GPU

### –î–ª—è T4 (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π Colab, ~16 GB –ø–∞–º—è—Ç–∏)

```yaml
hidden_size: 512
num_hidden_layers: 8
batch_size: 2
max_seq_length: 512
```

### –î–ª—è P100 (Pro Colab, ~40 GB)

```yaml
hidden_size: 768
num_hidden_layers: 12
batch_size: 4
max_seq_length: 1024
```

### –î–ª—è A100 (Premium Colab, ~80 GB)

```yaml
hidden_size: 1024
num_hidden_layers: 16
batch_size: 8
max_seq_length: 2048
```

---

## üêõ –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

| –ü—Ä–æ–±–ª–µ–º–∞ | –†–µ—à–µ–Ω–∏–µ |
|----------|---------|
| CUDA out of memory | –£–º–µ–Ω—å—à–∏—Ç–µ `batch_size` –∏–ª–∏ `max_seq_length` |
| –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ | –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU (`torch.cuda.is_available()`) |
| –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è | –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –Ω–∞—Ö–æ–¥–∏—Ç–µ—Å—å –≤ –ø–∞–ø–∫–µ `Mind-4-demo-code` |
| –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö | –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª `train_data.txt` –≤ Colab (–Ø—á–µ–π–∫–∞ 2) |

---

## –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è —Å–∫–∞—á–∞–π—Ç–µ —Ñ–∞–π–ª—ã –∏–∑ `./checkpoints/`:

```python
# –ë—ã—Å—Ç—Ä–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ Google Drive
!mkdir -p /content/gdrive/My\ Drive/mind-4-checkpoints
!cp ./checkpoints/*.pt /content/gdrive/My\ Drive/mind-4-checkpoints/
```

---

## –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

–í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –≤—ã –±—É–¥–µ—Ç–µ –≤–∏–¥–µ—Ç—å:

```
==================================================
–≠–ø–æ—Ö–∞ 1/1
==================================================
–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [3:45<00:00, 2.23 batches/s]
Train Loss: 3.2145
```

Loss –¥–æ–ª–∂–µ–Ω **—É–±—ã–≤–∞—Ç—å** - —ç—Ç–æ —Ö–æ—Ä–æ—à–∏–π –∑–Ω–∞–∫! üìà

---

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è

1. **–ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ:**
   ```python
   import torch
   checkpoint = torch.load('mind_epoch_1.pt')
   model.load_state_dict(checkpoint['model_state_dict'])
   ```

2. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:**
   ```bash
   python model/generate.py ./checkpoints/mind_epoch_1.pt --prompt "Hello"
   ```

3. **–£–ª—É—á—à–∞–π—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏ –¥–∞–Ω–Ω—ã–µ:**
   - –ë–æ–ª—å—à–µ —Å–ª–æ–µ–≤ = –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ (–Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ)
   - –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ = –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
   - –ë–æ–ª—å—à–µ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ = –ª—É—á—à–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å, –Ω–æ –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

---

## –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è

- **–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π Colab –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ 12 —á–∞—Å–æ–≤** - —Å–æ—Ö—Ä–∞–Ω—è–π—Ç–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ!
- **RAM –æ–±–Ω—É–ª—è–µ—Ç—Å—è** –ø—Ä–∏ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ - –∑–∞–Ω–æ–≤–æ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É
- **–õ—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏** –ø—Ä–∏ –ø–µ—Ä–≤–æ–º —Ç–µ—Å—Ç–µ
- **–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏** - –∫–∞–∂–¥—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —É–Ω–∏–∫–∞–ª–µ–Ω
